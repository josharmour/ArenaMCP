#!/usr/bin/env python3
"""
ArenaMCP Interactive Setup Wizard

Guides users through environment setup: venv creation, dependency installation,
LLM backend selection (proxy or Ollama), and .env configuration.

Runs with system Python (no venv needed). Uses only stdlib modules.
"""

import json
import os
import shutil
import subprocess
import sys
import textwrap
import urllib.error
import urllib.request
from pathlib import Path

# ── Constants ────────────────────────────────────────────────────────────────

ROOT = Path(__file__).resolve().parent
VENV_DIR = ROOT / "venv"
IS_WIN = sys.platform == "win32"
PIP_PATH = VENV_DIR / ("Scripts" if IS_WIN else "bin") / ("pip.exe" if IS_WIN else "pip")
PYTHON_PATH = VENV_DIR / ("Scripts" if IS_WIN else "bin") / ("python.exe" if IS_WIN else "python")
ENV_FILE = ROOT / ".env"
MTGA_LOG_DEFAULT = (
    Path(os.environ.get("APPDATA", "")) / "LocalLow" / "Wizards Of The Coast" / "MTGA" / "Player.log"
    if IS_WIN else Path.home() / ".wine" / "MTGA" / "Player.log"  # unlikely but placeholder
)

PROXY_DEFAULT_URL = "http://127.0.0.1:8080/v1"
PROXY_DEFAULT_KEY = "your-api-key-1"

BACKEND_PROXY = "proxy"
BACKEND_OLLAMA = "ollama"


# ── Helpers ──────────────────────────────────────────────────────────────────

def print_header(step_num: int, title: str) -> None:
    """Print a colored section header."""
    bar = "\u2500" * (50 - len(title) - 1)
    print(f"\n[{step_num}] {title.upper()} {bar}")


def prompt_choice(options: list[str], prompt_text: str = "Choice") -> int:
    """Show a numbered menu and return the 1-based selection."""
    while True:
        try:
            raw = input(f"\n    {prompt_text} [{'/'.join(str(i+1) for i in range(len(options)))}]: ").strip()
            idx = int(raw)
            if 1 <= idx <= len(options):
                return idx
        except (ValueError, EOFError):
            pass
        print(f"    Please enter a number between 1 and {len(options)}.")


def prompt_input(label: str, default: str = "") -> str:
    """Prompt for text input with an optional default."""
    suffix = f" [{default}]" if default else ""
    try:
        raw = input(f"    {label}{suffix}: ").strip()
    except EOFError:
        raw = ""
    return raw or default


def prompt_yn(label: str, default: bool = False) -> bool:
    """Yes/No prompt."""
    hint = "[y/N]" if not default else "[Y/n]"
    try:
        raw = input(f"    {label} {hint}: ").strip().lower()
    except EOFError:
        raw = ""
    if not raw:
        return default
    return raw.startswith("y")


def ok(msg: str) -> None:
    print(f"    \u2713 {msg}")


def fail(msg: str) -> None:
    print(f"    \u2717 {msg}")


def info(msg: str) -> None:
    print(f"    {msg}")


def run_pip(args: list[str], capture: bool = False) -> subprocess.CompletedProcess:
    """Run pip inside the venv."""
    cmd = [str(PIP_PATH)] + args
    if capture:
        return subprocess.run(cmd, capture_output=True, text=True, cwd=str(ROOT))
    return subprocess.run(cmd, cwd=str(ROOT))


def check_url(url: str, timeout: int = 3) -> bool:
    """Return True if a GET to url succeeds."""
    try:
        req = urllib.request.Request(url, method="GET")
        with urllib.request.urlopen(req, timeout=timeout):
            return True
    except Exception:
        return False


def read_env(path: Path) -> dict[str, str]:
    """Parse a .env file into a dict, preserving order via dict."""
    data: dict[str, str] = {}
    if not path.exists():
        return data
    with open(path) as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith("#") and "=" in line:
                key, _, value = line.partition("=")
                data[key.strip()] = value.strip()
    return data


def write_env(path: Path, data: dict[str, str]) -> None:
    """Write a dict as a .env file with a header comment."""
    lines = [
        "# ArenaMCP Configuration",
        "# Generated by setup_wizard.py",
        "",
    ]
    for key, value in data.items():
        lines.append(f"{key}={value}")
    lines.append("")  # trailing newline
    with open(path, "w") as f:
        f.write("\n".join(lines))


# ── Steps ────────────────────────────────────────────────────────────────────

def step_check_python() -> bool:
    """Step 1: Verify Python version."""
    print_header(1, "Check Python")
    v = sys.version_info
    if v < (3, 10):
        fail(f"Python {v.major}.{v.minor}.{v.micro} — version 3.10+ required")
        info("Please install Python 3.10+ from https://python.org")
        return False
    ok(f"Python {v.major}.{v.minor}.{v.micro}")
    return True


def step_virtual_environment() -> bool:
    """Step 2: Create or reuse venv, upgrade pip."""
    print_header(2, "Virtual Environment")

    if VENV_DIR.exists() and PIP_PATH.exists():
        ok("venv/ exists")
    else:
        info("Creating venv...")
        result = subprocess.run(
            [sys.executable, "-m", "venv", str(VENV_DIR)],
            capture_output=True, text=True,
        )
        if result.returncode != 0:
            fail("Failed to create virtual environment")
            if result.stderr:
                info(result.stderr.strip())
            return False
        ok("venv/ created")

    # Activate internally for subprocess calls
    if IS_WIN:
        scripts = str(VENV_DIR / "Scripts")
    else:
        scripts = str(VENV_DIR / "bin")
    os.environ["VIRTUAL_ENV"] = str(VENV_DIR)
    os.environ["PATH"] = scripts + os.pathsep + os.environ.get("PATH", "")

    info("Upgrading pip...")
    result = run_pip(["install", "--upgrade", "pip"], capture=True)
    if result.returncode == 0:
        ok("pip upgraded")
    else:
        # Non-fatal — pip may already be current
        info("pip upgrade skipped (may already be up to date)")

    return True


def step_install_dependencies() -> bool:
    """Step 3: Install packages from pyproject.toml and extras."""
    print_header(3, "Install Dependencies")

    info("Installing core + voice + LLM packages...")
    result = run_pip(["install", "-e", ".[full]"])
    if result.returncode != 0:
        fail("Some packages from pyproject.toml failed")
        info("Trying base install only...")
        run_pip(["install", "-e", "."])

    # Install extras from requirements.txt not covered by pyproject.toml
    extras = ["textual", "openai", "websocket-client", "scipy", "pygetwindow", "Pillow"]
    info("Installing additional packages...")
    result = run_pip(["install"] + extras)
    if result.returncode != 0:
        fail("Some additional packages failed (non-fatal)")
    else:
        ok("All packages installed")

    return True


def step_choose_backend() -> str:
    """Step 4: Let user pick LLM backend. Returns BACKEND_PROXY or BACKEND_OLLAMA."""
    print_header(4, "Choose LLM Backend")

    info("How will you connect to an LLM?\n")
    print(textwrap.dedent("""\
        [1] CLI Proxy (recommended)
            Routes to Claude, Gemini, GPT via local proxy server.
            Requires: cli-proxy-api running on localhost:8080
            Best for: Multiple model access, no API keys needed

        [2] Ollama (local, free)
            Run models locally on your GPU. No internet needed.
            Requires: Ollama installed + model pulled
            Best for: Privacy, offline use, free
    """))

    choice = prompt_choice(["CLI Proxy", "Ollama"])
    return BACKEND_PROXY if choice == 1 else BACKEND_OLLAMA


def step_backend_setup_proxy() -> bool:
    """Step 5 (proxy path): Check if proxy is reachable."""
    info("Checking localhost:8080...")
    if check_url(f"{PROXY_DEFAULT_URL}/models"):
        ok("Proxy is reachable")
        # Try to list models
        try:
            req = urllib.request.Request(f"{PROXY_DEFAULT_URL}/models")
            with urllib.request.urlopen(req, timeout=5) as resp:
                body = json.loads(resp.read().decode())
                models = [m.get("id", "?") for m in body.get("data", [])]
                if models:
                    info(f"Available models: {', '.join(models[:5])}")
        except Exception:
            pass
        return True
    else:
        fail("Not reachable")
        print()
        info("cli-proxy-api is not running. To set it up:")
        info("  1. Install: npm install -g cli-proxy-api")
        info("     (or see https://github.com/anthropics/cli-proxy-api)")
        info("  2. Run:     cli-proxy-api --port 8080")
        info("  3. Re-run this wizard to verify")
        print()
        return prompt_yn("Continue anyway?", default=False)


def step_backend_setup_ollama() -> bool:
    """Step 5 (Ollama path): Check Ollama install, offer to pull a model."""
    ollama_bin = shutil.which("ollama")
    if not ollama_bin:
        fail("Ollama not found on PATH")
        print()
        info("Install Ollama from https://ollama.ai")
        info("After installing, re-run this wizard.")
        print()
        return prompt_yn("Continue anyway?", default=False)

    # Check version
    try:
        ver = subprocess.run(
            ["ollama", "--version"], capture_output=True, text=True, timeout=5,
        )
        version_str = ver.stdout.strip() or "unknown"
        ok(f"Ollama found ({version_str})")
    except Exception:
        ok("Ollama found")

    # Check models
    try:
        result = subprocess.run(
            ["ollama", "list"], capture_output=True, text=True, timeout=10,
        )
        lines = [l for l in result.stdout.strip().splitlines() if l and not l.startswith("NAME")]
        if lines:
            ok(f"{len(lines)} model(s) available")
            for line in lines[:5]:
                info(f"  {line.split()[0]}")
            return True
        else:
            fail("No models found")
    except Exception:
        fail("Could not list models")

    print()
    if prompt_yn("Pull llama3.2 (recommended for coaching)?", default=True):
        info("Pulling llama3.2 — this may take a while...")
        pull = subprocess.run(["ollama", "pull", "llama3.2"])
        if pull.returncode == 0:
            ok("llama3.2 ready")
        else:
            fail("Pull failed — you can retry manually: ollama pull llama3.2")
    return True


def step_backend_setup(backend: str) -> bool:
    """Step 5: Validate the chosen backend."""
    print_header(5, "Backend Setup")

    if backend == BACKEND_PROXY:
        return step_backend_setup_proxy()
    else:
        return step_backend_setup_ollama()


def step_configuration(backend: str) -> None:
    """Step 6: Write .env file."""
    print_header(6, "Configuration")

    existing = read_env(ENV_FILE)
    if existing:
        info(f"Found existing .env with {len(existing)} setting(s) — merging")

    env = dict(existing)  # start from existing values

    if backend == BACKEND_PROXY:
        url_default = env.get("PROXY_BASE_URL", PROXY_DEFAULT_URL)
        key_default = env.get("PROXY_API_KEY", PROXY_DEFAULT_KEY)
        env["PROXY_BASE_URL"] = prompt_input("Proxy URL", url_default)
        env["PROXY_API_KEY"] = prompt_input("Proxy API key", key_default)
    # Ollama needs no env vars

    info("")
    info("Voice input mode:")
    info("  ptt  — Push-to-Talk (hold F4 to speak)")
    info("  vox  — Voice Activity Detection (auto)")
    info("  none — No voice input")
    voice_default = env.get("VOICE_MODE", "ptt")
    voice = prompt_input("Voice mode", voice_default)
    while voice not in ("ptt", "vox", "none"):
        info("Please enter ptt, vox, or none.")
        voice = prompt_input("Voice mode", voice_default)
    if voice != "none":
        env["VOICE_MODE"] = voice

    info("Writing .env file...")
    write_env(ENV_FILE, env)
    ok(".env written")


def step_verify(backend: str) -> None:
    """Step 7: Quick connectivity and path checks."""
    print_header(7, "Verify")

    # Test backend
    if backend == BACKEND_PROXY:
        info("Testing backend connection...")
        if check_url(f"{PROXY_DEFAULT_URL}/models"):
            ok("Proxy responds")
        else:
            fail("Proxy not reachable (you can start it later)")
    else:
        info("Testing backend connection...")
        try:
            result = subprocess.run(
                ["ollama", "list"], capture_output=True, text=True, timeout=5,
            )
            if result.returncode == 0:
                ok("Ollama responds")
            else:
                fail("Ollama not responding (you can start it later)")
        except Exception:
            fail("Ollama not responding (you can start it later)")

    # Test MTGA log path
    info("Testing MTGA log path...")
    if MTGA_LOG_DEFAULT.exists():
        ok(f"Player.log found at {MTGA_LOG_DEFAULT}")
    else:
        fail(f"Player.log not found at {MTGA_LOG_DEFAULT}")
        info("This is normal if MTGA hasn't been run yet.")
        info("The coach will find it automatically when MTGA starts.")

    # Success banner
    print()
    print("    " + "\u2550" * 44)
    print("    Setup complete! Run the coach with:")
    print("      coach.bat")
    print("    " + "\u2550" * 44)
    print()


# ── Main ─────────────────────────────────────────────────────────────────────

def main() -> int:
    print()
    print("=" * 52)
    print("  ArenaMCP Setup Wizard")
    print("=" * 52)

    # Step 1
    if not step_check_python():
        return 1

    # Step 2
    if not step_virtual_environment():
        return 1

    # Step 3
    if not step_install_dependencies():
        return 1

    # Step 4
    backend = step_choose_backend()

    # Step 5
    if not step_backend_setup(backend):
        info("Setup cancelled.")
        return 1

    # Step 6
    step_configuration(backend)

    # Step 7
    step_verify(backend)

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print("\n\n    Setup cancelled.")
        sys.exit(1)

---
phase: 05-voice-input
plan: 03
type: execute
---

<objective>
Integrate faster-whisper transcription and create unified voice input API.

Purpose: Complete the voice input pipeline by adding speech-to-text and providing a simple API for PTT/VOX transcription.
Output: Whisper transcription module and VoiceInput class that ties together audio capture, triggers, and STT.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@src/arenamcp/audio.py (from 05-01)
@src/arenamcp/triggers.py (from 05-02)

**Prior plan context:**
- 05-01: AudioRecorder with start_recording/stop_recording returning numpy array
- 05-02: PTTHandler (F4 hotkey) and VOXDetector (RMS threshold) with callbacks

**Tech decisions:**
- faster-whisper for transcription (CTranslate2 backend, 4x faster than OpenAI whisper)
- "base" model as default (good speed/accuracy tradeoff, ~150MB)
- CPU inference (no CUDA dependency for broader compatibility)
- Lazy model loading (avoid startup delay)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Whisper transcription module</name>
  <files>pyproject.toml (or requirements.txt), src/arenamcp/transcription.py</files>
  <action>
Add faster-whisper to dependencies.

Create src/arenamcp/transcription.py with:

WhisperTranscriber class:
- __init__(model_size: str = "base", device: str = "cpu", compute_type: str = "int8")
- transcribe(audio: np.ndarray, sample_rate: int = 16000) -> str: Transcribe audio to text
- _ensure_model_loaded(): Lazy-load model on first transcribe call

Internal state:
- _model: Optional[WhisperModel] - loaded model instance
- _model_size, _device, _compute_type: config for lazy loading

Transcription:
- Use model.transcribe(audio) with beam_size=5, language="en" (optimize for English)
- Concatenate all segment texts with spaces
- Return empty string for silence/no speech

IMPORTANT: Model download happens on first use (~150MB for base). Document this in docstring.
Use compute_type="int8" on CPU for best speed without GPU.
  </action>
  <verify>
python -c "
import numpy as np
from arenamcp.transcription import WhisperTranscriber

print('Initializing transcriber (model downloads on first use)...')
t = WhisperTranscriber(model_size='base')

# Test with silence (should return empty or minimal)
silence = np.zeros(16000, dtype=np.float32)  # 1 second silence
result = t.transcribe(silence)
print(f'Silence transcription: \"{result}\"')

# Test with simple tone (not speech, but exercises the pipeline)
tone = np.sin(np.linspace(0, 440*2*np.pi, 16000)).astype(np.float32) * 0.3
result = t.transcribe(tone)
print(f'Tone transcription: \"{result}\"')

print('OK - transcription pipeline works')
"
  </verify>
  <done>WhisperTranscriber can transcribe numpy audio arrays to text using faster-whisper</done>
</task>

<task type="auto">
  <name>Task 2: Create unified VoiceInput API</name>
  <files>src/arenamcp/voice.py, src/arenamcp/__init__.py</files>
  <action>
Create src/arenamcp/voice.py with:

VoiceInput class - unified interface for voice capture and transcription:
- __init__(mode: str = "ptt", ptt_key: str = "f4", vox_threshold: float = 0.02, vox_silence: float = 1.0)
- start(): Start listening (PTT hotkey or VOX continuous monitoring)
- stop(): Stop listening, cleanup resources
- wait_for_speech() -> str: Block until speech captured and transcribed, return text
- _on_recording_start(): Start AudioRecorder
- _on_recording_stop(): Stop AudioRecorder, transcribe, store result

Internal components:
- _recorder: AudioRecorder
- _transcriber: WhisperTranscriber (lazy-loaded)
- _trigger: PTTHandler or VOXDetector (based on mode)
- _result_ready: threading.Event
- _last_result: str

Mode handling:
- "ptt": Use PTTHandler, record while key held
- "vox": Use VOXDetector, continuous audio monitoring with process_audio calls

For VOX mode, run a background thread that:
1. Continuously reads audio chunks from a stream
2. Passes chunks to VOXDetector.process_audio()
3. When voice detected, accumulates audio until silence
4. Transcribes accumulated audio

Add to __init__.py: Export VoiceInput class.
  </action>
  <verify>
python -c "
from arenamcp.voice import VoiceInput

# Test PTT mode initialization
vi = VoiceInput(mode='ptt', ptt_key='f4')
print('VoiceInput (PTT mode) created')
vi.start()
print('Started - F4 hotkey active')
vi.stop()
print('Stopped')

# Test VOX mode initialization
vi2 = VoiceInput(mode='vox', vox_threshold=0.02)
print('VoiceInput (VOX mode) created')

print('OK - VoiceInput API ready')
"
  </verify>
  <done>VoiceInput class provides unified API for PTT and VOX voice capture with transcription</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `python -c "from arenamcp.voice import VoiceInput"` succeeds
- [ ] WhisperTranscriber can transcribe audio arrays
- [ ] VoiceInput can be instantiated in both PTT and VOX modes
- [ ] All new modules exported from arenamcp package
</verification>

<success_criteria>
- WhisperTranscriber with lazy model loading and int8 CPU inference
- VoiceInput unified API supporting both PTT and VOX modes
- Complete pipeline: hotkey/voice detection -> audio capture -> transcription -> text
- Phase 5 goal achieved: "Global PTT (F4 hotkey) and VOX capture with Whisper STT"
</success_criteria>

<output>
After completion, create `.planning/phases/05-voice-input/05-03-SUMMARY.md`
</output>
